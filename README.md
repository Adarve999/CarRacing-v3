<p align="center">
  <img src="images/ppoLSTM.gif" alt="CarRacing-v3 banner" width="50%"/>
</p>

<p align="center">
  <a href="https://www.python.org/"><img alt="Python" src="https://img.shields.io/badge/python-3.11+-3776AB?logo=python"></a>
  <a href="https://github.com/DLR-RM/stable-baselines3"><img alt="Stable-Baselines3" src="https://img.shields.io/badge/SB3-2.6.0-009688?logo=pytorch"></a>
  <a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/license-MIT-green.svg"></a>
</p>

# Conducci√≥n Aut√≥noma en **CarRacing-v3** üèéÔ∏è

Este proyecto explora c√≥mo distintos enfoques de *Deep Reinforcement Learning* resuelven la tarea de conducir un coche lo m√°s r√°pido posible sin salirse de pista en **CarRacing-v3** (Gymnasium).
El entorno devuelve exclusivamente p√≠xeles y un espacio de **5 acciones discretas**, de modo que cada algoritmo debe:

1. **Interpretar la escena visual** en tiempo real mediante una CNN.
2. **Decidir** c√≥mo frenar, derrapar o enderezar el coche con tan solo 5 movimientos posibles.
3. **Generalizar** a nuevos circuitos generados aleatoriamente en cada episodio.

Para poner a prueba distintas filosof√≠as de RL incluimos:

* **Double DQN** ‚Äì aprendizaje basado en valores que reutiliza cada fotograma cientos de veces a trav√©s de un *replay buffer*.
* **PPO (CNN)** ‚Äì gradiente de pol√≠tica con *clipping* que prima la estabilidad y el paralelismo masivo.
* **Recurrent PPO** ‚Äì PPO + LSTM: a√±ade memoria a corto plazo y mejora la trazada en curvas enlazadas.
* **TRPO** ‚Äì actualizaciones dentro de una regi√≥n de confianza (KL-constraint); ideal para estudiar convergencia sin saltos bruscos.

Todos los experimentos se entrenan durante **1 M de pasos** y guardan dos vueltas de demostraci√≥n en MP4 para comparar estilos de conducci√≥n.

---

## Ejecuci√≥n local (v√≠a Visual Studio Code)

> Todo el entrenamiento, la evaluaci√≥n y la grabaci√≥n de v√≠deos se orquestan desde el notebook `src/CarRacing-v3-Practice.ipynb`.
> Solo necesitas crear el entorno Conda y abrir Visual Studio Code.

#### 1 ¬∑ Clona el repositorio

```bash
git clone https://github.com/Adarve999/CarRacing-v3.git
cd CarRacing-v3
```

#### 2 ¬∑ Crea y activa el entorno

```bash
conda env create -f requirements.yml
conda activate rl_env
```

#### 3 ¬∑ Abre el proyecto en Visual Studio Code

* Abre el notebook **`src/CarRacing-v3-Practice.ipynb`**.
* Elige **rl_env** (el entorno Conda que acabas de crear), donde pone arriba a la derecha *Select Kernel*.
* Ejecuta las celdas en orden:

  1. entrena el algoritmo seleccionado,
  2. eval√∫a la recompensa media,
  3. graba dos vueltas en `videos/<modelo>/`,
  4. muestra los v√≠deos incrustados.

#### 4 ¬∑ Sigue el entrenamiento con TensorBoard

En una ventana de la terminal de conda, lanza el siguiente comando para monitorear el entrenamiento.
```bash
tensorboard --logdir logs/tensorboard
```

Los gr√°ficos se actualizar√°n en tiempo real mientras las celdas de entrenamiento est√°n corriendo.

---

# Conclusiones

| Algoritmo         | Recompensa media ¬± œÉ (10 episodios) | Observaciones (versi√≥n ‚Äúno t√©cnica‚Äù)                                                              |
| ----------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------- |
| **Double DQN**    | **874 ¬± 33**                        | Repasa cada curva hasta bordarla y acaba firmando la mejor vuelta.       |
| **Recurrent PPO** | 830 ¬± 44                            | Con memoria a corto plazo, anticipa los derrapes y mantiene una trazada muy limpia.               |
| **PPO**           | 809 ¬± 109                           | El todoterreno: aprende r√°pido pero, sin memoria, a veces pierde la l√≠nea ideal en curvas largas. |
| **TRPO**          | 711 ¬± 191                           | Conduce con extrema prudencia: casi nunca se sale, pero tampoco bate tiempos.                     |

## ¬øPor qu√© cada algoritmo rinde como rinde en *CarRacing-v3*?

* **Double DQN**
  Va guardando en su libreta millones de im√°genes y repas√°ndolas una y otra vez hasta pulir la maniobra exacta en cada curva.
  Como solo puede elegir entre 5 movimientos, le resulta f√°cil comparar y quedarse con el mejor.  Por eso acaba siendo el m√°s r√°pido.

* **Recurrent PPO**
  Conduce ‚Äúde o√≠do‚Äù: adem√°s de ver la pista, recuerda lo que acaba de pasar gracias a una memoria interna.
  Ese recuerdo le permite prever derrapes y corregir la direcci√≥n con antelaci√≥n, de ah√≠ que se acerque mucho a DQN aunque necesite algo m√°s de tiempo y de GPU.

* **PPO**
  Es la versi√≥n sin memoria.  Toma decisiones s√≥lidas y aprende deprisa porque practica con ocho coches a la vez, pero s√≥lo ‚Äúve‚Äù los √∫ltimos cuatro fotogramas; si la curva es larga puede reaccionar tarde y perder puntos.

* **TRPO**
  Conduce con mucha prudencia: se asegura de que cada cambio en su forma de pilotar sea peque√±o para no estrellarse por sorpresa.
  Esa cautela hace la conducci√≥n muy estable, pero tambi√©n le impide arriesgar cuando har√≠a falta; por eso su puntuaci√≥n se queda m√°s baja.

---

# Authors

* **Rub√©n Adarve P√©rez**
* **Javier Miranda**
* **Pablo de la Fuente**

Please use this bibtex if you want to cite this repository (main branch) in your publications:

```bibtex
@misc{CarRacingRL2025,
  author       = {Rub√©n Adarve P√©rez and Javier Miranda and Pablo de la Fuente},
  title        = {Conducci√≥n aut√≥noma en CarRacing-v3 con Deep RL},
  year         = {2025},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/Adarve999/CarRacing-v3}},
}
```
